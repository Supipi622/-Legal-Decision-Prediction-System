{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1a1zbQzek7kogVIpRjeFkblxSXOoEnKHL","timestamp":1682243939798}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install cleantext\n","!pip install sinling"],"metadata":{"id":"HC11MnClj4kR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import sys\n","import nltk\n","import string\n","import itertools\n","import numpy as np \n","import pandas as pd \n","import seaborn as sns\n","from sklearn import tree\n","from sklearn.svm import SVC\n","from joblib import dump, load\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","from cleantext import clean\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report\n","from sinling import SinhalaTokenizer\n","tokenizer = SinhalaTokenizer()\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from keras.models import Sequential,Model\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import LabelEncoder\n","from keras.utils.np_utils import to_categorical\n","from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.layers import Dense,LSTM, SpatialDropout1D, Embedding\n","\n","def prepocess_comment(comment):\n","    stop_set = list(string.punctuation)\n","    regrex_pattern = re.compile(pattern=\"[\"\n","                                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                                \"]+\", flags=re.UNICODE)\n","\n","    processed_comment = re.sub('@[^\\s]+', '', comment)\n","    processed_comment = re.sub(r'@\\w+', '', processed_comment)\n","    processed_comment = regrex_pattern.sub(r'', processed_comment)\n","    processed_comment = re.sub('http[^\\s]+', '', processed_comment)\n","    processed_comment = ''.join(\n","        c for c in processed_comment if not c.isnumeric())\n","    processed_comment = ' '.join(\n","        [i for i in tokenizer.tokenize(processed_comment) if i not in stop_set])\n","    # processed_comment = clean(processed_comment, to_ascii=False, lower=False, normalize_whitespace=True, no_line_breaks=True, strip_lines=True, keep_two_line_breaks=False, no_urls=True, no_emails=True, no_phone_numbers=True, no_numbers=True,\n","    #                           no_digits=True, no_currency_symbols=True, no_punct=True, no_emoji=True, replace_with_url='', replace_with_email='', replace_with_phone_number='', replace_with_number='', replace_with_digit='', replace_with_currency_symbol='', replace_with_punct='')\n","\n","    return processed_comment"],"metadata":{"id":"OPSQfMpUjujb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                              cmap=plt.cm.Greens):\n","    plt.figure(figsize=(50, 20), dpi=130)\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"],"metadata":{"id":"28yOtfmPkhc7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q4Te5id-IKtn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Load dataset\n","df = pd.read_excel(\"/content/english.xlsx\")\n","\n","print(df.isnull().sum())\n","nltk.download('wordnet')\n","nltk.download('stopwords')"],"metadata":{"id":"fzGB6XNjkxK5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df['cleaned'] = df['topic'].apply(lambda x: prepocess_comment(x))\n","df['cleaned'] = df['topic'].apply(lambda x:x.lower())\n","for i in range(len(df)):\n","    lw=[]\n","    for j in df['cleaned'][i].split():\n","        if len(j)>=3:                  \n","            lw.append(j)\n","    df['cleaned'][i]=\" \".join(lw)\n","ps = list(\";?.:!,\")\n","\n","df['cleaned'] = df['cleaned']\n","\n","for p in ps:   \n","    df['cleaned'] = df['cleaned'].str.replace(p, '')\n","\n","df['cleaned'] = df['cleaned'].str.replace(\"    \", \" \")\n","df['cleaned'] = df['cleaned'].str.replace('\"', '')\n","df['cleaned'] = df['cleaned'].apply(lambda x: x.replace('\\t', ' '))\n","df['cleaned'] = df['cleaned'].str.replace(\"'s\", \"\")\n","df['cleaned'] = df['cleaned'].apply(lambda x: x.replace('\\n', ' '))\n","\n","wl = WordNetLemmatizer()\n","nr = len(df)\n","lis = []\n","for r in range(0, nr):\n","    ll = []\n","    t = df.loc[r]['cleaned']\n","    tw = str(t).split(\" \")\n","    for w in tw:\n","        ll.append(wl.lemmatize(w, pos=\"v\"))\n","    lt = \" \".join(ll)\n","    lis.append(lt)\n","\n","df['cleaned'] = lis\n","\n","sw = list(stopwords.words('english'))\n","for s in sw:\n","    rs = r\"\\b\" + s + r\"\\b\"\n","    df['cleaned'] = df['cleaned'].str.replace(rs, '')\n","\n","print(df['cleaned'].values)"],"metadata":{"id":"s10_ILd1lWrc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier\n","\n","\n","# Separate questions and answers\n","questions = df['cleaned'].values\n","answers = df['explanation'].values\n","\n","# print(type(questions))\n","# print(answers)\n","\n","# # Initialize vectorizer and classifier\n","vectorizer = TfidfVectorizer()\n","classifier = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42)\n","\n","# Vectorize questions\n","X = vectorizer.fit_transform(questions)\n","\n","# # Train classifier\n","classifier.fit(X, answers)\n","yp=classifier.predict(X)\n","acc = accuracy_score(answers, yp)\n","print(\"accuracy is: \",acc)\n","\n","# Save model and vectorizer\n","# with open('model.pkl', 'wb') as f:\n","#     pickle.dump(classifier, f)\n","    \n","# with open('vectorizer.pkl', 'wb') as f:\n","#     pickle.dump(vectorizer, f)\n","?\n","new_question=\" what are the cases revavand early child hood maintenance  \"\n","\n","new_question_vec=vectorizer.transform([new_question])\n","\n","prediced_answer = classifier.predict(new_question_vec)\n","print(prediced_answer[0])\n","\n","# Save model and vectorizer\n","# with open('model.pkl', 'wb') as f:\n","#     pickle.dump(classifier, f)\n","    \n","# with open('vectorizer.pkl', 'wb') as f:\n","#     pickle.dump(vectorizer, f)\n"],"metadata":{"id":"7t0UVkb9blvh","executionInfo":{"status":"ok","timestamp":1682253255349,"user_tz":-330,"elapsed":414,"user":{"displayName":"Supipi Nelumika","userId":"17935337457710485236"}},"outputId":"6ff7adec-8b0b-4e4b-ebf1-ac22bb1c5906","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy is:  0.8656716417910447\n","Illegitimate child means a child born out of wedlock or born out of wedlock or adopted under the age of 18.\n","\n","Adopted child means a child \"adopted by a lawful district court. That means a child or children adopted under the Adoption Ordinance or the Declaration and Provisions of the Upland Law.\"\n"]}]},{"cell_type":"code","source":["!pip install pyrebase\n","!pip install firebase"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNG2Ol18IOwA","executionInfo":{"status":"ok","timestamp":1683262752694,"user_tz":-330,"elapsed":66085,"user":{"displayName":"Supipi Nelumika","userId":"17935337457710485236"}},"outputId":"b38c6147-2c28-4512-c312-8ed270ddf75d"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyrebase\n","  Downloading Pyrebase-3.0.27-py3-none-any.whl (9.6 kB)\n","Collecting requests-toolbelt==0.7.0\n","  Downloading requests_toolbelt-0.7.0-py2.py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-jwt==2.0.1\n","  Downloading python_jwt-2.0.1-py2.py3-none-any.whl (8.8 kB)\n","Collecting oauth2client==3.0.0\n","  Downloading oauth2client-3.0.0.tar.gz (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gcloud==0.17.0\n","  Downloading gcloud-0.17.0.tar.gz (458 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.0/458.0 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pycryptodome==3.4.3\n","  Downloading pycryptodome-3.4.3.tar.gz (6.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting requests==2.11.1\n","  Downloading requests-2.11.1-py2.py3-none-any.whl (514 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m514.8/514.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (0.21.0)\n","Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (1.59.0)\n","Requirement already satisfied: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (3.20.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gcloud==0.17.0->pyrebase) (1.16.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client==3.0.0->pyrebase) (0.5.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client==3.0.0->pyrebase) (0.3.0)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client==3.0.0->pyrebase) (4.9)\n","Collecting jws>=0.1.3\n","  Downloading jws-0.1.3.tar.gz (8.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->gcloud==0.17.0->pyrebase) (3.0.9)\n","Building wheels for collected packages: gcloud, oauth2client, pycryptodome, jws\n","  Building wheel for gcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gcloud: filename=gcloud-0.17.0-py3-none-any.whl size=638011 sha256=36e8e2a3a9661d790d73a99047981acc2cb42aa1238b3d465ac780fbf543923c\n","  Stored in directory: /root/.cache/pip/wheels/c5/7f/0b/0bd775c9e7f572be68ff9d285c17f2c497e7176235f897cb20\n","  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for oauth2client: filename=oauth2client-3.0.0-py3-none-any.whl size=106373 sha256=1addce8067733af6574dd8c6d119e0359cad34128aed9d8fa5f32e868afdfc26\n","  Stored in directory: /root/.cache/pip/wheels/da/55/05/62cddd6f3a70f9551e75efc49aafc0a4e91a77760fb83632d9\n","  Building wheel for pycryptodome (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycryptodome: filename=pycryptodome-3.4.3-cp310-cp310-linux_x86_64.whl size=6849527 sha256=7dfd4d13bbff0c4a62d8313705b8f6689169e4b0deeea866845d793e4609422d\n","  Stored in directory: /root/.cache/pip/wheels/1c/8d/f5/17e414ab6a18afa63e3a0c46a4912960edc6af44b7b70b68b6\n","  Building wheel for jws (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jws: filename=jws-0.1.3-py3-none-any.whl size=9412 sha256=7c648da5da75591ba322d36dc080d400ac94b62946fef80ed569c4af02cd4939\n","  Stored in directory: /root/.cache/pip/wheels/bf/f1/f9/add57b12d06f2d00045e86f6d5b2163aea577a8cf2f65c8367\n","Successfully built gcloud oauth2client pycryptodome jws\n","Installing collected packages: requests, pycryptodome, jws, requests-toolbelt, python-jwt, oauth2client, gcloud, pyrebase\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.27.1\n","    Uninstalling requests-2.27.1:\n","      Successfully uninstalled requests-2.27.1\n","  Attempting uninstall: oauth2client\n","    Found existing installation: oauth2client 4.1.3\n","    Uninstalling oauth2client-4.1.3:\n","      Successfully uninstalled oauth2client-4.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","yfinance 0.2.18 requires requests>=2.26, but you have requests 2.11.1 which is incompatible.\n","tweepy 4.13.0 requires requests<3,>=2.27.0, but you have requests 2.11.1 which is incompatible.\n","tensorflow-datasets 4.8.3 requires requests>=2.19.0, but you have requests 2.11.1 which is incompatible.\n","tensorboard 2.12.2 requires requests<3,>=2.21.0, but you have requests 2.11.1 which is incompatible.\n","spacy 3.5.2 requires requests<3.0.0,>=2.13.0, but you have requests 2.11.1 which is incompatible.\n","pydrive 1.3.1 requires oauth2client>=4.0.0, but you have oauth2client 3.0.0 which is incompatible.\n","pooch 1.6.0 requires requests>=2.19.0, but you have requests 2.11.1 which is incompatible.\n","pandas-datareader 0.10.0 requires requests>=2.19.0, but you have requests 2.11.1 which is incompatible.\n","google-colab 1.0.0 requires requests>=2.27.0, but you have requests 2.11.1 which is incompatible.\n","google-cloud-storage 2.8.0 requires requests<3.0.0dev,>=2.18.0, but you have requests 2.11.1 which is incompatible.\n","google-cloud-bigquery 3.9.0 requires requests<3.0.0dev,>=2.21.0, but you have requests 2.11.1 which is incompatible.\n","google-api-core 2.11.0 requires requests<3.0.0dev,>=2.18.0, but you have requests 2.11.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gcloud-0.17.0 jws-0.1.3 oauth2client-3.0.0 pycryptodome-3.4.3 pyrebase-3.0.27 python-jwt-2.0.1 requests-2.11.1 requests-toolbelt-0.7.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting firebase\n","  Downloading firebase-4.0.1-py3-none-any.whl (12 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firebase) (2.11.1)\n","Installing collected packages: firebase\n","Successfully installed firebase-4.0.1\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"yco6GEoTJAJp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFzElP3QRKoY"},"outputs":[],"source":["# Import necessary libraries\n","from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n","import pandas as pd\n","\n","# Load dataset\n","df = pd.read_csv('dataset.csv')\n","\n","# Initialize tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n","\n","# Define pipeline for training\n","qa_pipeline = pipeline(\n","    \"question-answering\",\n","    model=model,\n","    tokenizer=tokenizer,\n",")\n","\n","# Train model\n","for i in range(len(df)):\n","    question = df.iloc[i]['question']\n","    context = df.iloc[i]['context']\n","    answer = df.iloc[i]['answer']\n","    \n","    result = qa_pipeline({\n","        'context': context,\n","        'question': question\n","    })\n","    \n","    # Calculate loss and update model\n","    loss = abs(len(answer) - len(result['answer']))\n","    model.backward(loss)\n","    model.step()\n","\n","# Save trained model\n","model.save_pretrained('trained-model')\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"2S1aPs35DyVW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering\n","\n","# Load trained model\n","tokenizer = AutoTokenizer.from_pretrained(\"path/to/trained-model\")\n","model = AutoModelForQuestionAnswering.from_pretrained(\"path/to/trained-model\")\n","\n","# Define pipeline for prediction\n","qa_pipeline = pipeline(\n","    \"question-answering\",\n","    model=model,\n","    tokenizer=tokenizer,\n",")\n","\n","# Predict answer\n","context = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\"\n","question = \"Who is a disabled child of marriage?\"\n","result = qa_pipeline({\n","    'context': context,\n","    'question': question\n","})\n","\n","# Print answer\n","print(result['answer'])"],"metadata":{"id":"oojxLLIERMP_"},"execution_count":null,"outputs":[]}]}